"""
Individual Vulnerability Profiles Analysis
============================================
Identifies vulnerable vs resilient males with high loneliness

Profiles:
1. Vulnerable: UCLA > median & WCST PE > 75th percentile
2. Resilient: UCLA > median & WCST PE < 25th percentile
3. Control: UCLA ≤ median

Compares groups on:
- DASS (Depression, Anxiety, Stress)
- Age, education
- All EF metrics (PRP τ/σ, Stroop interference, etc.)
- Logistic regression: Predictors of vulnerability
"""

import sys
if sys.platform.startswith("win") and hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# Configuration
# ============================================================================

RESULTS_DIR = Path("results")
OUTPUT_DIR = Path("results/analysis_outputs/individual_profiles")
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

print("=" * 80)
print("INDIVIDUAL VULNERABILITY PROFILES ANALYSIS")
print("=" * 80)

# ============================================================================
# Load Data
# ============================================================================

print("\nLoading data...")

# Participant info and surveys
participants = pd.read_csv(RESULTS_DIR / "1_participants_info.csv")
participants = participants.rename(columns={'participantId': 'participant_id'})

surveys = pd.read_csv(RESULTS_DIR / "2_surveys_results.csv")
surveys = surveys.rename(columns={'participantId': 'participant_id'})

# UCLA scores
if 'surveyName' in surveys.columns:
    ucla_data = surveys[surveys['surveyName'].str.lower() == 'ucla'].copy()
elif 'survey' in surveys.columns:
    ucla_data = surveys[surveys['survey'].str.lower() == 'ucla'].copy()
else:
    raise KeyError("No survey name column found")
ucla_scores = ucla_data.groupby('participant_id')['score'].sum().reset_index()
ucla_scores.columns = ['participant_id', 'ucla_total']

# DASS scores
if 'surveyName' in surveys.columns:
    dass_data = surveys[surveys['surveyName'].str.lower().str.contains('dass')].copy()
elif 'survey' in surveys.columns:
    dass_data = surveys[surveys['survey'].str.lower().str.contains('dass')].copy()
else:
    dass_data = pd.DataFrame()
dass_scores = dass_data.groupby(['participant_id', 'questionText'])['score'].sum().unstack(fill_value=0)

# Map DASS subscales
dep_items = [col for col in dass_scores.columns if any(x in str(col).lower() for x in ['meaningless', 'nothing', 'enthused', 'worth', 'positive', 'initiative', 'future'])]
anx_items = [col for col in dass_scores.columns if any(x in str(col).lower() for x in ['breathing', 'trembling', 'worried', 'panic', 'heart', 'scared', 'dry'])]
stress_items = [col for col in dass_scores.columns if any(x in str(col).lower() for x in ['wind down', 'over-react', 'nervous', 'agitated', 'relax', 'intolerant', 'touchy'])]

dass_summary = pd.DataFrame()
dass_summary['participant_id'] = dass_scores.index
dass_summary['dass_depression'] = dass_scores[dep_items].sum(axis=1).values if dep_items else 0
dass_summary['dass_anxiety'] = dass_scores[anx_items].sum(axis=1).values if anx_items else 0
dass_summary['dass_stress'] = dass_scores[stress_items].sum(axis=1).values if stress_items else 0
dass_summary['dass_total'] = dass_summary[['dass_depression', 'dass_anxiety', 'dass_stress']].sum(axis=1)
dass_summary = dass_summary.reset_index(drop=True)

# Merge with demographics
master = participants[['participant_id', 'age', 'gender', 'education']].merge(
    ucla_scores, on='participant_id', how='inner'
).merge(
    dass_summary, on='participant_id', how='left'
)

# WCST data for PE rate
wcst_trials = pd.read_csv(RESULTS_DIR / "4b_wcst_trials.csv")
if 'participantId' in wcst_trials.columns:
    wcst_trials = wcst_trials.rename(columns={'participantId': 'participant_id'})

import ast
def _parse_wcst_extra(extra_str):
    if not isinstance(extra_str, str):
        return {}
    try:
        return ast.literal_eval(extra_str)
    except (ValueError, SyntaxError):
        return {}

wcst_trials['extra_dict'] = wcst_trials['extra'].apply(_parse_wcst_extra)
wcst_trials['is_pe'] = wcst_trials['extra_dict'].apply(lambda x: x.get('isPE', False))

wcst_summary = wcst_trials.groupby('participant_id').agg(
    pe_count=('is_pe', 'sum'),
    total_trials=('is_pe', 'count'),
    wcst_accuracy=('correct', lambda x: (x.sum() / len(x)) * 100),
    wcst_mean_rt=('rt', 'mean'),
    wcst_sd_rt=('rt', 'std')
).reset_index()
wcst_summary['pe_rate'] = (wcst_summary['pe_count'] / wcst_summary['total_trials']) * 100

# PRP Ex-Gaussian
prp_exg = pd.read_csv("results/analysis_outputs/mechanism_analysis/exgaussian/prp_exgaussian_parameters.csv")
if prp_exg['participant_id'].dtype == 'O' and prp_exg['participant_id'].iloc[0].startswith('\ufeff'):
    prp_exg['participant_id'] = prp_exg['participant_id'].str.replace('\ufeff', '')
prp_exg['prp_tau_long'] = prp_exg['long_tau']
prp_exg['prp_sigma_long'] = prp_exg['long_sigma']
prp_exg['prp_tau_short'] = prp_exg['short_tau']
prp_exg['prp_sigma_bottleneck'] = prp_exg['short_sigma'] - prp_exg['long_sigma']

# Stroop Ex-Gaussian
stroop_exg = pd.read_csv("results/analysis_outputs/mechanism_analysis/exgaussian/exgaussian_parameters.csv")
if stroop_exg['participant_id'].dtype == 'O' and stroop_exg['participant_id'].iloc[0].startswith('\ufeff'):
    stroop_exg['participant_id'] = stroop_exg['participant_id'].str.replace('\ufeff', '')

# Stroop interference
stroop_trials = pd.read_csv(RESULTS_DIR / "4c_stroop_trials.csv")
if 'participantId' in stroop_trials.columns:
    stroop_trials = stroop_trials.rename(columns={'participantId': 'participant_id'})

stroop_summary = stroop_trials[stroop_trials['timeout'] == False].groupby(['participant_id', 'condition']).agg(
    rt_mean=('rt', 'mean')
).reset_index()
stroop_wide = stroop_summary.pivot(index='participant_id', columns='condition', values='rt_mean').reset_index()
stroop_wide.columns.name = None
if 'incongruent' in stroop_wide.columns and 'congruent' in stroop_wide.columns:
    stroop_wide['stroop_interference'] = stroop_wide['incongruent'] - stroop_wide['congruent']

# PRP bottleneck
prp_trials = pd.read_csv(RESULTS_DIR / "4a_prp_trials.csv")
if 'participantId' in prp_trials.columns:
    prp_trials = prp_trials.rename(columns={'participantId': 'participant_id'})

prp_trials = prp_trials[
    (prp_trials['t1_timeout'] == False) &
    (prp_trials['t2_timeout'] == False) &
    (prp_trials['t2_rt'] > 0)
].copy()

def bin_soa(soa):
    if soa <= 150:
        return 'short'
    elif soa >= 1200:
        return 'long'
    else:
        return 'other'

prp_trials['soa_bin'] = prp_trials['soa'].apply(bin_soa)
prp_trials = prp_trials[prp_trials['soa_bin'].isin(['short', 'long'])].copy()

prp_rt = prp_trials.groupby(['participant_id', 'soa_bin'])['t2_rt'].mean().unstack().reset_index()
if 'short' in prp_rt.columns and 'long' in prp_rt.columns:
    prp_rt['prp_bottleneck'] = prp_rt['short'] - prp_rt['long']

# Merge all
master = master.merge(wcst_summary[['participant_id', 'pe_rate', 'wcst_accuracy', 'wcst_sd_rt']],
                      on='participant_id', how='left')
master = master.merge(prp_exg[['participant_id', 'prp_tau_long', 'prp_sigma_long',
                                 'prp_tau_short', 'prp_sigma_bottleneck']],
                      on='participant_id', how='left')
master = master.merge(stroop_exg[['participant_id', 'tau', 'sigma']],
                      on='participant_id', how='left', suffixes=('', '_stroop'))
if 'stroop_interference' in stroop_wide.columns:
    master = master.merge(stroop_wide[['participant_id', 'stroop_interference']],
                          on='participant_id', how='left')
if 'prp_bottleneck' in prp_rt.columns:
    master = master.merge(prp_rt[['participant_id', 'prp_bottleneck']],
                          on='participant_id', how='left')

# Create gender dummy
master['gender_male'] = (master['gender'].str.lower() == 'male').astype(int)

# Drop missing
master = master.dropna(subset=['ucla_total', 'gender_male', 'pe_rate']).copy()

print(f"  Loaded {len(master)} participants with complete data")
print(f"  Males: {master['gender_male'].sum()}, Females: {(1-master['gender_male']).sum()}\n")

# ============================================================================
# Define Profiles (Males Only)
# ============================================================================

print("=" * 80)
print("DEFINING VULNERABILITY PROFILES (Males Only)")
print("=" * 80)

males = master[master['gender_male'] == 1].copy()
print(f"\nTotal males: {len(males)}")

# Calculate thresholds
ucla_median = males['ucla_total'].median()
pe_75 = males['pe_rate'].quantile(0.75)
pe_25 = males['pe_rate'].quantile(0.25)

print(f"\nThresholds:")
print(f"  UCLA median: {ucla_median:.1f}")
print(f"  PE 75th percentile: {pe_75:.1f}%")
print(f"  PE 25th percentile: {pe_25:.1f}%")

# Assign profiles
def assign_profile(row):
    if row['ucla_total'] > ucla_median:
        if row['pe_rate'] > pe_75:
            return 'Vulnerable'
        elif row['pe_rate'] < pe_25:
            return 'Resilient'
        else:
            return 'Moderate'
    else:
        return 'Control'

males['profile'] = males.apply(assign_profile, axis=1)

# Count profiles
profile_counts = males['profile'].value_counts()
print(f"\nProfile distribution:")
for profile, count in profile_counts.items():
    print(f"  {profile}: {count}")

# Filter to key groups for comparison
key_groups = males[males['profile'].isin(['Vulnerable', 'Resilient', 'Control'])].copy()

print(f"\nComparing {len(key_groups)} males across 3 profiles:")
print(f"  Vulnerable: {(key_groups['profile']=='Vulnerable').sum()}")
print(f"  Resilient: {(key_groups['profile']=='Resilient').sum()}")
print(f"  Control: {(key_groups['profile']=='Control').sum()}")

# ============================================================================
# Compare Groups
# ============================================================================

print("\n" + "=" * 80)
print("GROUP COMPARISONS")
print("=" * 80)

comparison_vars = [
    'ucla_total', 'pe_rate', 'wcst_accuracy', 'wcst_sd_rt',
    'dass_depression', 'dass_anxiety', 'dass_stress', 'dass_total',
    'age', 'prp_tau_long', 'prp_sigma_long', 'prp_tau_short',
    'prp_sigma_bottleneck', 'tau', 'sigma', 'stroop_interference', 'prp_bottleneck'
]

# Descriptive stats by profile
desc_stats = []
for var in comparison_vars:
    if var not in key_groups.columns:
        continue

    for profile in ['Vulnerable', 'Resilient', 'Control']:
        data = key_groups[key_groups['profile'] == profile][var].dropna()
        if len(data) > 0:
            desc_stats.append({
                'variable': var,
                'profile': profile,
                'n': len(data),
                'mean': data.mean(),
                'sd': data.std(),
                'median': data.median(),
                'min': data.min(),
                'max': data.max()
            })

desc_df = pd.DataFrame(desc_stats)

# ANOVA/Kruskal-Wallis tests
anova_results = []
for var in comparison_vars:
    if var not in key_groups.columns:
        continue

    vulnerable = key_groups[key_groups['profile'] == 'Vulnerable'][var].dropna()
    resilient = key_groups[key_groups['profile'] == 'Resilient'][var].dropna()
    control = key_groups[key_groups['profile'] == 'Control'][var].dropna()

    if len(vulnerable) >= 3 and len(resilient) >= 3 and len(control) >= 3:
        # Kruskal-Wallis (non-parametric)
        h_stat, p_val = stats.kruskal(vulnerable, resilient, control)

        # Effect size (eta-squared approximation)
        # Using sum of ranks
        n_total = len(vulnerable) + len(resilient) + len(control)
        eta_sq = (h_stat - 2) / (n_total - 3) if n_total > 3 else np.nan

        anova_results.append({
            'variable': var,
            'test': 'Kruskal-Wallis',
            'h_statistic': h_stat,
            'p_value': p_val,
            'eta_squared_approx': eta_sq,
            'vulnerable_mean': vulnerable.mean(),
            'resilient_mean': resilient.mean(),
            'control_mean': control.mean()
        })

anova_df = pd.DataFrame(anova_results)

# Post-hoc pairwise comparisons (Mann-Whitney U) for significant variables
posthoc_results = []
for _, row in anova_df[anova_df['p_value'] < 0.10].iterrows():  # Liberal threshold for exploratory
    var = row['variable']

    vulnerable = key_groups[key_groups['profile'] == 'Vulnerable'][var].dropna()
    resilient = key_groups[key_groups['profile'] == 'Resilient'][var].dropna()
    control = key_groups[key_groups['profile'] == 'Control'][var].dropna()

    # Vulnerable vs Resilient
    if len(vulnerable) >= 3 and len(resilient) >= 3:
        u_stat, p_val = stats.mannwhitneyu(vulnerable, resilient, alternative='two-sided')
        r_effect = abs(u_stat / (len(vulnerable) * len(resilient)) - 0.5) * 2  # Effect size r
        posthoc_results.append({
            'variable': var,
            'comparison': 'Vulnerable vs Resilient',
            'u_statistic': u_stat,
            'p_value': p_val,
            'effect_size_r': r_effect,
            'group1_mean': vulnerable.mean(),
            'group2_mean': resilient.mean(),
            'mean_diff': vulnerable.mean() - resilient.mean()
        })

    # Vulnerable vs Control
    if len(vulnerable) >= 3 and len(control) >= 3:
        u_stat, p_val = stats.mannwhitneyu(vulnerable, control, alternative='two-sided')
        r_effect = abs(u_stat / (len(vulnerable) * len(control)) - 0.5) * 2
        posthoc_results.append({
            'variable': var,
            'comparison': 'Vulnerable vs Control',
            'u_statistic': u_stat,
            'p_value': p_val,
            'effect_size_r': r_effect,
            'group1_mean': vulnerable.mean(),
            'group2_mean': control.mean(),
            'mean_diff': vulnerable.mean() - control.mean()
        })

    # Resilient vs Control
    if len(resilient) >= 3 and len(control) >= 3:
        u_stat, p_val = stats.mannwhitneyu(resilient, control, alternative='two-sided')
        r_effect = abs(u_stat / (len(resilient) * len(control)) - 0.5) * 2
        posthoc_results.append({
            'variable': var,
            'comparison': 'Resilient vs Control',
            'u_statistic': u_stat,
            'p_value': p_val,
            'effect_size_r': r_effect,
            'group1_mean': resilient.mean(),
            'group2_mean': control.mean(),
            'mean_diff': resilient.mean() - control.mean()
        })

posthoc_df = pd.DataFrame(posthoc_results)

# Print key findings
print("\n" + "=" * 80)
print("KEY FINDINGS: Variables Distinguishing Profiles")
print("=" * 80)

if len(anova_df) > 0:
    sig_vars = anova_df[anova_df['p_value'] < 0.05].sort_values('p_value')

    if len(sig_vars) > 0:
        print("\nSignificant group differences (p < 0.05):")
        for _, row in sig_vars.iterrows():
            print(f"\n  {row['variable']}:")
            print(f"    H = {row['h_statistic']:.2f}, p = {row['p_value']:.4f}")
            print(f"    Vulnerable: {row['vulnerable_mean']:.2f}")
            print(f"    Resilient: {row['resilient_mean']:.2f}")
            print(f"    Control: {row['control_mean']:.2f}")
    else:
        print("\nNo significant group differences at p < 0.05")

    trend_vars = anova_df[(anova_df['p_value'] >= 0.05) & (anova_df['p_value'] < 0.10)].sort_values('p_value')
    if len(trend_vars) > 0:
        print("\n\nTrend-level differences (0.05 ≤ p < 0.10):")
        for _, row in trend_vars.iterrows():
            print(f"\n  {row['variable']}:")
            print(f"    H = {row['h_statistic']:.2f}, p = {row['p_value']:.4f}")
            print(f"    Vulnerable: {row['vulnerable_mean']:.2f}")
            print(f"    Resilient: {row['resilient_mean']:.2f}")
            print(f"    Control: {row['control_mean']:.2f}")

# ============================================================================
# Logistic Regression: Vulnerable vs Resilient
# ============================================================================

print("\n\n" + "=" * 80)
print("LOGISTIC REGRESSION: Predicting Vulnerability (Vulnerable vs Resilient)")
print("=" * 80)

# Filter to vulnerable vs resilient only
vul_res = key_groups[key_groups['profile'].isin(['Vulnerable', 'Resilient'])].copy()
vul_res['is_vulnerable'] = (vul_res['profile'] == 'Vulnerable').astype(int)

# Select predictors
predictor_cols = ['dass_depression', 'dass_anxiety', 'dass_stress', 'age',
                  'prp_tau_long', 'prp_sigma_bottleneck', 'wcst_sd_rt']

# Remove missing
vul_res_clean = vul_res.dropna(subset=predictor_cols + ['is_vulnerable']).copy()

if len(vul_res_clean) >= 10:
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler

    X = vul_res_clean[predictor_cols].values
    y = vul_res_clean['is_vulnerable'].values

    # Standardize
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Fit logistic regression
    log_reg = LogisticRegression(penalty=None, max_iter=1000)
    log_reg.fit(X_scaled, y)

    # Get coefficients
    coefs = pd.DataFrame({
        'predictor': predictor_cols,
        'coefficient': log_reg.coef_[0],
        'odds_ratio': np.exp(log_reg.coef_[0])
    }).sort_values('coefficient', key=abs, ascending=False)

    print(f"\nN = {len(vul_res_clean)} (Vulnerable: {y.sum()}, Resilient: {len(y) - y.sum()})")
    print("\nLogistic regression coefficients (standardized):")
    print(coefs.to_string(index=False))

    # Accuracy
    y_pred = log_reg.predict(X_scaled)
    accuracy = (y_pred == y).mean()
    print(f"\nModel accuracy: {accuracy:.2%}")

else:
    print(f"\nInsufficient data (N={len(vul_res_clean)}) for logistic regression")
    coefs = pd.DataFrame()

# ============================================================================
# Save Results
# ============================================================================

print("\n\nSaving results...")

# Profile assignments
profile_file = OUTPUT_DIR / "male_vulnerability_profiles.csv"
males[['participant_id', 'profile', 'ucla_total', 'pe_rate', 'wcst_accuracy',
       'dass_depression', 'dass_anxiety', 'dass_stress']].to_csv(
    profile_file, index=False, encoding='utf-8-sig'
)
print(f"  Profile assignments: {profile_file}")

# Descriptive stats
desc_file = OUTPUT_DIR / "profile_descriptive_stats.csv"
desc_df.to_csv(desc_file, index=False, encoding='utf-8-sig')
print(f"  Descriptive stats: {desc_file}")

# ANOVA results
anova_file = OUTPUT_DIR / "profile_group_comparisons.csv"
anova_df.to_csv(anova_file, index=False, encoding='utf-8-sig')
print(f"  Group comparisons: {anova_file}")

# Post-hoc tests
if len(posthoc_df) > 0:
    posthoc_file = OUTPUT_DIR / "profile_posthoc_tests.csv"
    posthoc_df.to_csv(posthoc_file, index=False, encoding='utf-8-sig')
    print(f"  Post-hoc tests: {posthoc_file}")

# Logistic regression
if len(coefs) > 0:
    logreg_file = OUTPUT_DIR / "vulnerability_logistic_regression.csv"
    coefs.to_csv(logreg_file, index=False, encoding='utf-8-sig')
    print(f"  Logistic regression: {logreg_file}")

print("\n" + "=" * 80)
print("INDIVIDUAL VULNERABILITY PROFILES ANALYSIS COMPLETE")
print("=" * 80)
