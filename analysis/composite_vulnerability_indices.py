"""
composite_vulnerability_indices.py

Creates gender-specific vulnerability indices based on the double dissociation pattern:
- Male Meta-Control Index: Perseveration, accuracy, lapses, variability
- Female Cascade/Interference Index: Incongruent accuracy, error cascades

Tests UCLA × Gender effects on these composite scores with DASS control.

Outputs:
- composite_indices.csv
- composite_regression_results.csv
- composite_by_ucla_quartile.png
"""

import sys
import pandas as pd
from data_loader_utils import load_master_dataset
import numpy as np
from pathlib import Path
import statsmodels.formula.api as smf
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# UTF-8 encoding for Windows
if sys.platform.startswith("win") and hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')

# Directories
RESULTS_DIR = Path("results")
OUTPUT_DIR = Path("results/analysis_outputs/advanced_comprehensive/composite_indices")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("=" * 80)
print("GENDER-SPECIFIC VULNERABILITY INDICES")
print("=" * 80)
print()

# ============================================================================
# 1. LOAD DATA
# ============================================================================

print("Loading data...")

# Core master (demographics + summary metrics)
master = load_master_dataset(use_cache=True, merge_cognitive_summary=True)
master = master.rename(columns={"gender_normalized": "gender"})
master["gender"] = master["gender"].fillna("").astype(str).str.strip().str.lower()
master["gender_male"] = master["gender"].map({"male": 1, "female": 0})

# Optional comprehensive features
comp_path = RESULTS_DIR / "analysis_outputs/master_comprehensive_features.csv"
if comp_path.exists():
    comp = pd.read_csv(comp_path, encoding='utf-8-sig')
    comp.columns = comp.columns.str.lower()
    if 'participantid' in comp.columns and 'participant_id' not in comp.columns:
        comp = comp.rename(columns={'participantid': 'participant_id'})
    master = master.merge(comp, on='participant_id', how='left')

# Trial-level derived features (for CV/PES)
trial_feat_path = RESULTS_DIR / "analysis_outputs/trial_level_features.csv"
if trial_feat_path.exists():
    trial_features = pd.read_csv(trial_feat_path, encoding='utf-8-sig')
    trial_features.columns = trial_features.columns.str.lower()
    if 'participantid' in trial_features.columns and 'participant_id' not in trial_features.columns:
        trial_features = trial_features.rename(columns={'participantid': 'participant_id'})
    master = master.merge(
        trial_features[['participant_id', 'prp_t2_cv_all', 'stroop_post_error_slowing']],
        on='participant_id', how='left'
    )

# Ex-Gaussian parameters (PRP)
exg_path = RESULTS_DIR / "analysis_outputs/prp_exgaussian_dass_controlled/prp_exgaussian_parameters.csv"
if exg_path.exists():
    exgaussian = pd.read_csv(exg_path, encoding='utf-8-sig')
    exgaussian.columns = exgaussian.columns.str.lower()
    if 'participantid' in exgaussian.columns and 'participant_id' not in exgaussian.columns:
        exgaussian = exgaussian.rename(columns={'participantid': 'participant_id'})
    master = master.merge(exgaussian[['participant_id', 'overall_tau', 'overall_sigma']],
                          on='participant_id', how='left')

print(f"  Total participants: {len(master)}")
print(f"  Male: {(master['gender'] == 'male').sum()}")
print(f"  Female: {(master['gender'] == 'female').sum()}")
print()

# ============================================================================
# 2. COMPUTE PRP T2 RT IQR (from CV)
# ============================================================================

print("Computing additional variability metrics...")

# IQR can be approximated from CV: IQR ≈ 1.35 * SD = 1.35 * (mean * CV)
# But we need trial-level data for exact IQR. For now, use CV as proxy.
# Higher CV = higher variability
master['prp_t2_variability'] = master['prp_t2_cv_all']

print(f"  PRP T2 variability available for {master['prp_t2_variability'].notna().sum()} participants")
print()

# ============================================================================
# 3. CREATE MALE META-CONTROL INDEX
# ============================================================================

print("Creating Male Meta-Control Index...")

# Variables for male index (higher = worse control):
# 1. WCST PE rate (↑ bad)
# 2. WCST accuracy (↓ bad → reverse)
# 3. PRP overall tau (↑ more lapses = bad)
# 4. PRP T2 variability (↑ bad)
# 5. Stroop PES (↑ over-control = bad)

male_vars = {
    'perseverative_error_rate': 1,   # Positive: higher is worse
    'wcst_accuracy': -1,              # Negative: lower is worse
    'overall_tau': 1,                 # Positive: longer tail is worse
    'prp_t2_variability': 1,          # Positive: higher variability is worse
    'stroop_post_error_slowing': 1    # Positive: excessive slowing is worse
}

# Standardize each variable
scaler = StandardScaler()
male_data = master[master['gender'] == 'male'].copy()

male_index_components = []
for var, direction in male_vars.items():
    if var in male_data.columns:
        valid_data = male_data[[var]].dropna()
        if len(valid_data) > 5:
            z_var = scaler.fit_transform(male_data[[var]])
            male_data[f'z_{var}'] = z_var
            male_index_components.append(f'z_{var}')
            print(f"  {var}: {male_data[var].notna().sum()} males, direction={'+' if direction > 0 else '-'}")

# Compute composite
if len(male_index_components) >= 3:
    male_data['male_metacontrol_index'] = male_data[male_index_components].sum(axis=1, skipna=False) / len(male_index_components)
    print(f"\n  Male Meta-Control Index: N={male_data['male_metacontrol_index'].notna().sum()}")
    print(f"  Mean={male_data['male_metacontrol_index'].mean():.3f}, SD={male_data['male_metacontrol_index'].std():.3f}")
else:
    print("  ERROR: Not enough variables for male index")
    male_data['male_metacontrol_index'] = np.nan

print()

# ============================================================================
# 4. CREATE FEMALE CASCADE/INTERFERENCE INDEX
# ============================================================================

print("Creating Female Cascade/Interference Index...")

# Variables for female index (higher = worse):
# 1. Stroop incongruent accuracy (↓ bad → reverse)
# 2. PRP error cascade rate (↑ bad)
# 3. PRP cascade inflation (↑ bad)

female_vars = {
    'stroop_incong_acc': -1,        # Negative: lower is worse
    'prp_cascade_rate': 1,          # Positive: higher is worse
    'prp_cascade_inflation': 1      # Positive: higher is worse
}

female_data = master[master['gender'] == 'female'].copy()

female_index_components = []
for var, direction in female_vars.items():
    if var in female_data.columns:
        valid_data = female_data[[var]].dropna()
        if len(valid_data) > 5:
            z_var = scaler.fit_transform(female_data[[var]])
            female_data[f'z_{var}'] = z_var
            female_index_components.append(f'z_{var}')
            print(f"  {var}: {female_data[var].notna().sum()} females, direction={'+' if direction > 0 else '-'}")

# Compute composite
if len(female_index_components) >= 2:
    female_data['female_cascade_index'] = female_data[female_index_components].sum(axis=1, skipna=False) / len(female_index_components)
    print(f"\n  Female Cascade Index: N={female_data['female_cascade_index'].notna().sum()}")
    print(f"  Mean={female_data['female_cascade_index'].mean():.3f}, SD={female_data['female_cascade_index'].std():.3f}")
else:
    print("  ERROR: Not enough variables for female index")
    female_data['female_cascade_index'] = np.nan

print()

# ============================================================================
# 5. CREATE UNIFIED COMPOSITE SCORE
# ============================================================================

print("Creating unified vulnerability index...")

# For regression: combine both indices into single "vulnerability score"
# Assign male_index to males, female_index to females

master_with_indices = master.copy()
master_with_indices['vulnerability_index'] = np.nan

# Male indices
male_ids = male_data['participant_id'].values
male_indices = male_data.set_index('participant_id')['male_metacontrol_index']
master_with_indices.loc[master_with_indices['participant_id'].isin(male_ids), 'vulnerability_index'] = \
    master_with_indices.loc[master_with_indices['participant_id'].isin(male_ids), 'participant_id'].map(male_indices)

# Female indices
female_ids = female_data['participant_id'].values
female_indices = female_data.set_index('participant_id')['female_cascade_index']
master_with_indices.loc[master_with_indices['participant_id'].isin(female_ids), 'vulnerability_index'] = \
    master_with_indices.loc[master_with_indices['participant_id'].isin(female_ids), 'participant_id'].map(female_indices)

print(f"  Unified vulnerability index: {master_with_indices['vulnerability_index'].notna().sum()} participants")
print()

# ============================================================================
# 6. STANDARDIZE PREDICTORS
# ============================================================================

print("Standardizing predictors...")

# Ensure required columns exist
required_cols = ['ucla_total', 'dass_depression', 'dass_anxiety', 'dass_stress', 'age', 'gender_male']
missing_cols = [col for col in required_cols if col not in master_with_indices.columns]
if missing_cols:
    print(f"  WARNING: Missing columns: {missing_cols}")

# Clean data
master_clean = master_with_indices.dropna(subset=['vulnerability_index', 'ucla_total', 'gender_male', 'age']).copy()

# Standardize
scaler = StandardScaler()
master_clean['z_age'] = scaler.fit_transform(master_clean[['age']])
master_clean['z_ucla'] = scaler.fit_transform(master_clean[['ucla_total']])

if 'dass_depression' in master_clean.columns:
    master_clean['z_dass_dep'] = scaler.fit_transform(master_clean[['dass_depression']])
    master_clean['z_dass_anx'] = scaler.fit_transform(master_clean[['dass_anxiety']])
    master_clean['z_dass_str'] = scaler.fit_transform(master_clean[['dass_stress']])
    dass_available = True
else:
    print("  WARNING: DASS variables not available")
    dass_available = False

print(f"  Clean sample: N={len(master_clean)}")
print()

# ============================================================================
# 7. HIERARCHICAL REGRESSION: VULNERABILITY ~ UCLA × GENDER + DASS
# ============================================================================

print("=" * 80)
print("HIERARCHICAL REGRESSION: VULNERABILITY INDEX")
print("=" * 80)
print()

results_list = []

if len(master_clean) >= 30 and dass_available:
    # Model 0: Covariates only
    formula0 = "vulnerability_index ~ z_age + C(gender_male)"
    model0 = smf.ols(formula0, data=master_clean).fit()

    # Model 1: + DASS
    formula1 = "vulnerability_index ~ z_age + C(gender_male) + z_dass_dep + z_dass_anx + z_dass_str"
    model1 = smf.ols(formula1, data=master_clean).fit()

    # Model 2: + UCLA
    formula2 = "vulnerability_index ~ z_age + C(gender_male) + z_dass_dep + z_dass_anx + z_dass_str + z_ucla"
    model2 = smf.ols(formula2, data=master_clean).fit()

    # Model 3: + UCLA × Gender
    formula3 = "vulnerability_index ~ z_age + C(gender_male) * z_ucla + z_dass_dep + z_dass_anx + z_dass_str"
    model3 = smf.ols(formula3, data=master_clean).fit()

    # Print summaries
    models = [model0, model1, model2, model3]
    model_names = ['Covariates', 'DASS', 'UCLA', 'UCLA×Gender']

    for i, (model, name) in enumerate(zip(models, model_names)):
        print(f"Model {i}: {name}")
        print(f"  R² = {model.rsquared:.3f}, Adj R² = {model.rsquared_adj:.3f}")
        print(f"  F = {model.fvalue:.2f}, p = {model.f_pvalue:.4f}")
        print()

        results_list.append({
            'model': f'Model {i}: {name}',
            'r_squared': model.rsquared,
            'adj_r_squared': model.rsquared_adj,
            'f_value': model.fvalue,
            'f_p': model.f_pvalue,
            'n': int(model.nobs)
        })

    # Focus on Model 3 (full model)
    print("=" * 80)
    print("MODEL 3 (FULL): VULNERABILITY ~ UCLA × GENDER + DASS + AGE")
    print("=" * 80)
    print()
    print(model3.summary())
    print()

    # Extract key coefficients
    interaction_term = 'C(gender_male)[T.1]:z_ucla'

    if interaction_term in model3.params:
        print("KEY FINDINGS:")
        print(f"  UCLA main effect: β={model3.params['z_ucla']:.3f}, p={model3.pvalues['z_ucla']:.4f}")
        print(f"  UCLA × Gender interaction: β={model3.params[interaction_term]:.3f}, p={model3.pvalues[interaction_term]:.4f}")
        print()
else:
    print("  WARNING: Insufficient data or missing DASS variables")
    print()

# Save regression results
results_df = pd.DataFrame(results_list)
results_df.to_csv(OUTPUT_DIR / "composite_regression_results.csv", index=False, encoding='utf-8-sig')
print(f"Saved: {OUTPUT_DIR / 'composite_regression_results.csv'}")
print()

# ============================================================================
# 8. UCLA QUARTILE ANALYSIS
# ============================================================================

print("=" * 80)
print("VULNERABILITY INDEX BY UCLA QUARTILE")
print("=" * 80)
print()

# Compute UCLA quartiles
master_clean['ucla_quartile'] = pd.qcut(master_clean['ucla_total'], q=4, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'], duplicates='drop')

# Summary by gender and quartile
summary_by_quartile = master_clean.groupby(['gender', 'ucla_quartile'])['vulnerability_index'].agg(['mean', 'std', 'count']).reset_index()
print(summary_by_quartile)
print()

# ============================================================================
# 9. VISUALIZATION
# ============================================================================

print("Creating visualizations...")

# Figure 1: Vulnerability Index by UCLA Quartile (Gender-separated)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Male
male_quartile = master_clean[master_clean['gender'] == 'male']
if len(male_quartile) > 0:
    sns.boxplot(data=male_quartile, x='ucla_quartile', y='vulnerability_index',
                color='steelblue', ax=axes[0])
    axes[0].set_title('Male Meta-Control Index by UCLA Quartile', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('UCLA Loneliness Quartile', fontsize=12)
    axes[0].set_ylabel('Meta-Control Vulnerability Index\n(higher = worse)', fontsize=12)
    axes[0].axhline(0, color='gray', linestyle='--', linewidth=1)
    axes[0].grid(axis='y', alpha=0.3)

# Female
female_quartile = master_clean[master_clean['gender'] == 'female']
if len(female_quartile) > 0:
    sns.boxplot(data=female_quartile, x='ucla_quartile', y='vulnerability_index',
                color='coral', ax=axes[1])
    axes[1].set_title('Female Cascade/Interference Index by UCLA Quartile', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('UCLA Loneliness Quartile', fontsize=12)
    axes[1].set_ylabel('Cascade/Interference Vulnerability Index\n(higher = worse)', fontsize=12)
    axes[1].axhline(0, color='gray', linestyle='--', linewidth=1)
    axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(OUTPUT_DIR / "composite_by_ucla_quartile.png", dpi=300, bbox_inches='tight')
print(f"Saved: {OUTPUT_DIR / 'composite_by_ucla_quartile.png'}")
plt.close()

# Figure 2: Scatter plot with regression lines
fig, ax = plt.subplots(figsize=(10, 6))

for gender, color, label in [('male', 'steelblue', 'Male Meta-Control'),
                               ('female', 'coral', 'Female Cascade/Interference')]:
    subset = master_clean[master_clean['gender'] == gender]
    if len(subset) > 0:
        ax.scatter(subset['ucla_total'], subset['vulnerability_index'],
                   color=color, alpha=0.6, s=60, label=label)

        # Fit line
        z = np.polyfit(subset['ucla_total'].values, subset['vulnerability_index'].values, 1)
        p = np.poly1d(z)
        x_line = np.linspace(subset['ucla_total'].min(), subset['ucla_total'].max(), 100)
        ax.plot(x_line, p(x_line), color=color, linewidth=2, linestyle='--')

ax.axhline(0, color='gray', linestyle='--', linewidth=1)
ax.set_xlabel('UCLA Loneliness Total Score', fontsize=12, fontweight='bold')
ax.set_ylabel('Vulnerability Index (z-score)', fontsize=12, fontweight='bold')
ax.set_title('Gender-Specific Vulnerability Indices vs UCLA Loneliness', fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig(OUTPUT_DIR / "vulnerability_ucla_scatter.png", dpi=300, bbox_inches='tight')
print(f"Saved: {OUTPUT_DIR / 'vulnerability_ucla_scatter.png'}")
plt.close()

# ============================================================================
# 10. SAVE COMPOSITE INDICES
# ============================================================================

print()
print("Saving composite indices...")

# Combine male and female data
indices_output = pd.concat([
    male_data[['participant_id', 'male_metacontrol_index']],
    female_data[['participant_id', 'female_cascade_index']]
], axis=0)

# Merge back to master
master_final = master.merge(indices_output, on='participant_id', how='left')
master_final = master_final.merge(
    master_with_indices[['participant_id', 'vulnerability_index']],
    on='participant_id', how='left'
)

# Save
output_file = OUTPUT_DIR / "composite_indices.csv"
master_final.to_csv(output_file, index=False, encoding='utf-8-sig')
print(f"Saved: {output_file}")
print()

print("=" * 80)
print("COMPOSITE VULNERABILITY INDICES COMPLETE")
print("=" * 80)
print()
print("KEY FILES:")
print(f"  - {OUTPUT_DIR / 'composite_indices.csv'}")
print(f"  - {OUTPUT_DIR / 'composite_regression_results.csv'}")
print(f"  - {OUTPUT_DIR / 'composite_by_ucla_quartile.png'}")
print(f"  - {OUTPUT_DIR / 'vulnerability_ucla_scatter.png'}")
print()
