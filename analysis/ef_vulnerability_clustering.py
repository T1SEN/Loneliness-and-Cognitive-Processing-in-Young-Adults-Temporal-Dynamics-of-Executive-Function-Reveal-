"""
Executive Function Vulnerability Clustering
============================================
Identifies cognitive subtypes based on EF performance patterns using unsupervised clustering.

Research Question: "Are there distinct EF vulnerability profiles, and do they map onto
loneliness/gender patterns?"

Hypothesis:
- Cluster 1: "Resilient" - Normal EF across all domains
- Cluster 2: "Lapse-heavy" (male-dominant) - High PRP tau/sigma, WCST PE
- Cluster 3: "Hypervigilant" (female-dominant) - Low tau, high variability, post-switch errors

Method:
1. Select EF features (z-scored)
2. K-means clustering (k=2-4, silhouette score for optimal k)
3. Compare clusters on UCLA, DASS, gender, age
4. Visualize with PCA/t-SNE

Note: EXPLORATORY analysis - not confirmatory hypothesis testing.

Author: Research Team
Date: 2025-01-16
"""

import sys
if sys.platform.startswith("win") and hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, silhouette_samples
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from analysis.utils.data_loader_utils import load_master_dataset
warnings.filterwarnings('ignore')

np.random.seed(42)

# Constants
MIN_N_ML = 30  # Minimum sample size for clustering (15 per cluster for k=2)

# Directories
RESULTS_DIR = Path("results")
OUTPUT_DIR = Path("results/analysis_outputs/ef_vulnerability_clustering")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("=" * 80)
print("EXECUTIVE FUNCTION VULNERABILITY CLUSTERING")
print("=" * 80)
print("\nPurpose: Identify EF vulnerability subtypes via unsupervised clustering")
print("Method: K-means + silhouette analysis + demographic comparison\n")

# Load master dataset
master = load_master_dataset(use_cache=True, merge_cognitive_summary=True)
# Use gender_normalized if available
if 'gender_normalized' in master.columns:
    master['gender'] = master['gender_normalized'].fillna('').astype(str).str.strip().str.lower()
else:
    master['gender'] = master['gender'].fillna('').astype(str).str.strip().str.lower()
if 'ucla_total' not in master.columns and 'ucla_score' in master.columns:
    master['ucla_total'] = master['ucla_score']
master['gender_male'] = (master['gender'] == 'male').astype(int)

print(f"\nData overview:")
print(f"  Total N: {len(master)}")

print(f"\nData overview:")
print(f"  Total N: {len(master)}")
if 'gender' in master.columns:
    print(f"  Males: {sum(master['gender']=='male')}, Females: {sum(master['gender']=='female')}")

print("\n" + "=" * 80)
print("STEP 1: Select EF Features for Clustering")
print("=" * 80)

# Define candidate features (comprehensive EF battery)
candidate_features = []

# WCST features
wcst_features = ['pe_rate', 'pe_rate', 'pe_rate',
                 'wcst_accuracy', 'wcst_mean_rt']
candidate_features.extend([f for f in wcst_features if f in master.columns])

# PRP features (focus on tau, mu, sigma for lapse detection)
prp_features = ['prp_tau_long', 'prp_mu_long', 'prp_sigma_long',
                'prp_tau_short', 'prp_mu_short', 'prp_sigma_short',
                'prp_bottleneck', 't2_rt_mean_long', 't2_rt_sd']
candidate_features.extend([f for f in prp_features if f in master.columns])

# Stroop features
stroop_features = ['stroop_interference', 'stroop_incongruent_rt',
                   'stroop_incongruent_acc', 'stroop_congruent_rt']
candidate_features.extend([f for f in stroop_features if f in master.columns])

# Variability/meta-control features
meta_features = ['rt_cv', 'rt_sd', 'pes', 'post_error_slowing',
                'post_switch_errors', 'switch_cost']
candidate_features.extend([f for f in meta_features if f in master.columns])

# Remove duplicates
candidate_features = list(set(candidate_features))

print(f"\nCandidate EF features found: {len(candidate_features)}")
for feat in sorted(candidate_features):
    print(f"  - {feat}")

# Clean data
feature_df = master[['participant_id', 'gender', 'gender_male', 'age'] + candidate_features].copy()

# Add UCLA and DASS if available
if 'ucla_total' in master.columns:
    feature_df['ucla_total'] = master['ucla_total']
if all(col in master.columns for col in ['dass_depression', 'dass_anxiety', 'dass_stress']):
    feature_df['dass_depression'] = master['dass_depression']
    feature_df['dass_anxiety'] = master['dass_anxiety']
    feature_df['dass_stress'] = master['dass_stress']

# Drop rows with missing EF features
feature_df = feature_df.dropna(subset=candidate_features)

print(f"\nAfter dropping missing features: N={len(feature_df)}")

if len(feature_df) < MIN_N_ML:
    print(f"ERROR: Insufficient data (N={len(feature_df)} < {MIN_N_ML}).")
    print("Clustering requires at least 20 participants.")
    sys.exit(1)

# Standardize features for clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(feature_df[candidate_features])

print(f"\nFinal clustering dataset: N={len(X_scaled)}, Features={X_scaled.shape[1]}")

print("\n" + "=" * 80)
print("STEP 2: Determine Optimal Number of Clusters")
print("=" * 80)

# Test k from 2 to 5 (or N/10, whichever is smaller)
# Ensure max_k is at least 3 to allow comparison between k=2 and k=3
max_k = max(3, min(5, len(feature_df) // 10))

if max_k < 2:
    print(f"ERROR: N={len(feature_df)} too small for clustering.")
    print("Need at least N=30 for meaningful clustering.")
    sys.exit(1)

k_range = range(2, max_k + 1)

silhouette_scores = []
inertias = []

print(f"\nTesting k from 2 to {max_k}:")

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)
    cluster_labels = kmeans.fit_predict(X_scaled)

    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    inertia = kmeans.inertia_

    silhouette_scores.append(silhouette_avg)
    inertias.append(inertia)

    print(f"  k={k}: Silhouette={silhouette_avg:.3f}, Inertia={inertia:.1f}")

# Select optimal k (highest silhouette)
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"\nOptimal k = {optimal_k} (highest silhouette score: {max(silhouette_scores):.3f})")

print("\n" + "=" * 80)
print(f"STEP 3: Fit Final K-Means Model (k={optimal_k})")
print("=" * 80)

# Fit final model
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
feature_df['cluster'] = kmeans_final.fit_predict(X_scaled)

print(f"\nCluster sizes:")
for cluster_id in range(optimal_k):
    cluster_size = sum(feature_df['cluster'] == cluster_id)
    print(f"  Cluster {cluster_id}: N={cluster_size} ({cluster_size/len(feature_df)*100:.1f}%)")

# Compute cluster centroids (in original feature space for interpretability)
cluster_profiles = feature_df.groupby('cluster')[candidate_features].mean()

print("\nCluster Centroids (top 5 features per cluster):")
for cluster_id in range(optimal_k):
    centroid = cluster_profiles.loc[cluster_id]
    top_features = centroid.abs().sort_values(ascending=False).head(5)

    print(f"\n  Cluster {cluster_id}:")
    for feat, val in top_features.items():
        print(f"    {feat}: {val:.3f}")

# Save cluster centroids
cluster_profiles.to_csv(OUTPUT_DIR / "cluster_centroids.csv", encoding='utf-8-sig')
print(f"\n✓ Saved: cluster_centroids.csv")

print("\n" + "=" * 80)
print("STEP 4: Compare Clusters on Demographics & Loneliness")
print("=" * 80)

# Demographic comparison
demographic_vars = ['age', 'gender_male']
if 'ucla_total' in feature_df.columns:
    demographic_vars.append('ucla_total')
if 'dass_depression' in feature_df.columns:
    demographic_vars.extend(['dass_depression', 'dass_anxiety', 'dass_stress'])

demographic_comparison = feature_df.groupby('cluster')[demographic_vars].agg(['mean', 'std', 'count'])

print("\nDemographic profiles by cluster:")
print(demographic_comparison)

# Statistical tests
print("\nStatistical tests:")

# Gender distribution (chi-square)
if 'gender_male' in demographic_vars:
    contingency_table = pd.crosstab(feature_df['cluster'], feature_df['gender_male'])
    chi2, p_chi, dof, expected = stats.chi2_contingency(contingency_table)
    print(f"\n  Gender × Cluster association:")
    print(f"    χ²({dof}) = {chi2:.3f}, p = {p_chi:.4f}")

    if p_chi < 0.05:
        print("    → SIGNIFICANT: Clusters differ by gender composition")
    else:
        print("    → NS: No gender clustering")

# UCLA comparison (ANOVA)
if 'ucla_total' in feature_df.columns:
    cluster_groups = [feature_df[feature_df['cluster'] == i]['ucla_total'].dropna()
                     for i in range(optimal_k)]
    f_stat, p_anova = stats.f_oneway(*cluster_groups)

    print(f"\n  UCLA across clusters:")
    print(f"    F({optimal_k-1}, {len(feature_df)-optimal_k}) = {f_stat:.3f}, p = {p_anova:.4f}")

    if p_anova < 0.05:
        print("    → SIGNIFICANT: Clusters differ in loneliness")
        # Post-hoc comparisons
        for i in range(optimal_k):
            cluster_ucla_mean = feature_df[feature_df['cluster'] == i]['ucla_total'].mean()
            print(f"      Cluster {i}: M={cluster_ucla_mean:.2f}")
    else:
        print("    → NS: No loneliness clustering")

# DASS comparison
if 'dass_depression' in feature_df.columns:
    for dass_var in ['dass_depression', 'dass_anxiety', 'dass_stress']:
        cluster_groups_dass = [feature_df[feature_df['cluster'] == i][dass_var].dropna()
                              for i in range(optimal_k)]
        f_stat_dass, p_anova_dass = stats.f_oneway(*cluster_groups_dass)

        print(f"\n  {dass_var.upper().replace('_', ' ')} across clusters:")
        print(f"    F = {f_stat_dass:.3f}, p = {p_anova_dass:.4f}")

# Save demographic comparison
demographic_comparison.to_csv(OUTPUT_DIR / "cluster_demographics.csv", encoding='utf-8-sig')
print(f"\n✓ Saved: cluster_demographics.csv")

print("\n" + "=" * 80)
print("STEP 5: Assign Cluster Labels (Interpretive Names)")
print("=" * 80)

# Heuristic cluster labeling based on EF profiles
cluster_labels_interpretive = {}

for cluster_id in range(optimal_k):
    cluster_data = feature_df[feature_df['cluster'] == cluster_id]

    # Check key features
    mean_pe = cluster_data[[col for col in candidate_features if 'pe' in col.lower()]].mean().mean()
    mean_tau = cluster_data[[col for col in candidate_features if 'tau' in col.lower()]].mean().mean()
    mean_sigma = cluster_data[[col for col in candidate_features if 'sigma' in col.lower()]].mean().mean()

    gender_pct_male = cluster_data['gender_male'].mean() * 100 if 'gender_male' in cluster_data.columns else 50

    ucla_mean = cluster_data['ucla_total'].mean() if 'ucla_total' in cluster_data.columns else np.nan

    # Simple labeling heuristic
    if not np.isnan(mean_tau) and mean_tau > 0.2:  # High tau (z-scored)
        label = "Lapse-Heavy"
    elif not np.isnan(mean_sigma) and mean_sigma > 0.2:  # High variability
        label = "High-Variability"
    elif not np.isnan(mean_pe) and mean_pe > 0.2:  # High PE
        label = "Perseverative"
    else:
        label = "Resilient"

    # Add gender skew if present
    if gender_pct_male > 70:
        label += " (Male-Dominant)"
    elif gender_pct_male < 30:
        label += " (Female-Dominant)"

    cluster_labels_interpretive[cluster_id] = label

    print(f"\nCluster {cluster_id}: {label}")
    print(f"  Gender: {gender_pct_male:.1f}% male")
    if not np.isnan(ucla_mean):
        print(f"  UCLA: M={ucla_mean:.2f}")
    print(f"  N={len(cluster_data)}")

# Add interpretive labels to dataframe
feature_df['cluster_label'] = feature_df['cluster'].map(cluster_labels_interpretive)

# Save cluster assignments
assignments_df = feature_df[['participant_id', 'cluster', 'cluster_label',
                             'gender', 'age'] +
                            (['ucla_total'] if 'ucla_total' in feature_df.columns else [])].copy()
assignments_df.to_csv(OUTPUT_DIR / "cluster_assignments.csv",
                     index=False, encoding='utf-8-sig')
print(f"\n✓ Saved: cluster_assignments.csv")

print("\n" + "=" * 80)
print("STEP 6: Visualizations")
print("=" * 80)

# 6A: PCA for 2D visualization
print("\nCreating PCA visualization...")

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

feature_df['pca1'] = X_pca[:, 0]
feature_df['pca2'] = X_pca[:, 1]

fig, ax = plt.subplots(figsize=(10, 7))

for cluster_id in range(optimal_k):
    cluster_data = feature_df[feature_df['cluster'] == cluster_id]
    ax.scatter(cluster_data['pca1'], cluster_data['pca2'],
              label=f"{cluster_labels_interpretive[cluster_id]} (N={len(cluster_data)})",
              s=100, alpha=0.7, edgecolors='black', linewidth=0.5)

ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)', fontsize=12)
ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)', fontsize=12)
ax.set_title('EF Vulnerability Clusters (PCA Projection)', fontsize=14, fontweight='bold')
ax.legend(loc='best', fontsize=10)
ax.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(OUTPUT_DIR / "cluster_pca_visualization.png", dpi=300, bbox_inches='tight')
plt.close()
print("✓ Saved: cluster_pca_visualization.png")

# 6B: Cluster profile heatmap (z-scored features)
print("\nCreating cluster profile heatmap...")

# Select top 15 most discriminative features
feature_variance_across_clusters = cluster_profiles.var(axis=0).sort_values(ascending=False)
top_discriminative = feature_variance_across_clusters.head(15).index.tolist()

heatmap_data = cluster_profiles[top_discriminative].T

fig, ax = plt.subplots(figsize=(10, len(top_discriminative) * 0.4 + 2))
sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdBu_r',
            center=0, cbar_kws={'label': 'Z-scored Mean'},
            linewidths=0.5, ax=ax,
            yticklabels=top_discriminative,
            xticklabels=[cluster_labels_interpretive[i] for i in range(optimal_k)])
ax.set_xlabel('Cluster', fontsize=11)
ax.set_ylabel('EF Feature', fontsize=11)
ax.set_title('Top 15 Discriminative Features by Cluster', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig(OUTPUT_DIR / "cluster_profile_heatmap.png", dpi=300, bbox_inches='tight')
plt.close()
print("✓ Saved: cluster_profile_heatmap.png")

# 6C: Demographics by cluster (bar plots)
print("\nCreating demographic comparison plots...")

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Gender composition
if 'gender_male' in demographic_vars:
    gender_pct = feature_df.groupby('cluster')['gender_male'].mean() * 100
    ax = axes[0]
    ax.bar(range(optimal_k), gender_pct, color='steelblue', edgecolor='black')
    ax.set_xticks(range(optimal_k))
    ax.set_xticklabels([cluster_labels_interpretive[i] for i in range(optimal_k)],
                       rotation=15, ha='right')
    ax.set_ylabel('% Male', fontsize=11)
    ax.set_title('Gender Composition by Cluster', fontsize=12, fontweight='bold')
    ax.axhline(50, color='red', linestyle='--', linewidth=1, label='50% (balanced)')
    ax.legend()

# UCLA loneliness
if 'ucla_total' in demographic_vars:
    ucla_means = feature_df.groupby('cluster')['ucla_total'].mean()
    ucla_sems = feature_df.groupby('cluster')['ucla_total'].sem()
    ax = axes[1]
    ax.bar(range(optimal_k), ucla_means, yerr=ucla_sems, color='coral',
          edgecolor='black', capsize=5)
    ax.set_xticks(range(optimal_k))
    ax.set_xticklabels([cluster_labels_interpretive[i] for i in range(optimal_k)],
                       rotation=15, ha='right')
    ax.set_ylabel('UCLA Loneliness (M ± SEM)', fontsize=11)
    ax.set_title('Loneliness by Cluster', fontsize=12, fontweight='bold')

# DASS depression (if available)
if 'dass_depression' in demographic_vars:
    dass_means = feature_df.groupby('cluster')['dass_depression'].mean()
    dass_sems = feature_df.groupby('cluster')['dass_depression'].sem()
    ax = axes[2]
    ax.bar(range(optimal_k), dass_means, yerr=dass_sems, color='mediumpurple',
          edgecolor='black', capsize=5)
    ax.set_xticks(range(optimal_k))
    ax.set_xticklabels([cluster_labels_interpretive[i] for i in range(optimal_k)],
                       rotation=15, ha='right')
    ax.set_ylabel('DASS Depression (M ± SEM)', fontsize=11)
    ax.set_title('Depression by Cluster', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig(OUTPUT_DIR / "cluster_demographics_plots.png", dpi=300, bbox_inches='tight')
plt.close()
print("✓ Saved: cluster_demographics_plots.png")

print("\n" + "=" * 80)
print("FINAL REPORT")
print("=" * 80)

# Create comprehensive report
report_path = OUTPUT_DIR / "CLUSTERING_REPORT.txt"
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("EF VULNERABILITY CLUSTERING REPORT\n")
    f.write("=" * 80 + "\n\n")

    f.write("PURPOSE\n")
    f.write("-" * 80 + "\n")
    f.write("Identify distinct EF vulnerability profiles using unsupervised clustering.\n")
    f.write("Test whether clusters map onto loneliness/gender patterns.\n\n")

    f.write("METHOD\n")
    f.write("-" * 80 + "\n")
    f.write(f"N = {len(feature_df)} participants\n")
    f.write(f"Features: {len(candidate_features)} EF metrics (z-scored)\n")
    f.write(f"Algorithm: K-means with k={optimal_k} (optimal via silhouette score)\n")
    f.write(f"Validation: Silhouette score = {max(silhouette_scores):.3f}\n\n")

    f.write("IDENTIFIED CLUSTERS\n")
    f.write("-" * 80 + "\n\n")

    for cluster_id in range(optimal_k):
        cluster_data = feature_df[feature_df['cluster'] == cluster_id]

        f.write(f"CLUSTER {cluster_id}: {cluster_labels_interpretive[cluster_id]}\n")
        f.write(f"  N = {len(cluster_data)} ({len(cluster_data)/len(feature_df)*100:.1f}%)\n")

        if 'gender_male' in cluster_data.columns:
            male_pct = cluster_data['gender_male'].mean() * 100
            f.write(f"  Gender: {male_pct:.1f}% male, {100-male_pct:.1f}% female\n")

        if 'age' in cluster_data.columns:
            age_mean = cluster_data['age'].mean()
            f.write(f"  Age: M={age_mean:.1f}\n")

        if 'ucla_total' in cluster_data.columns:
            ucla_mean = cluster_data['ucla_total'].mean()
            f.write(f"  UCLA Loneliness: M={ucla_mean:.2f}\n")

        if 'dass_depression' in cluster_data.columns:
            dass_mean = cluster_data['dass_depression'].mean()
            f.write(f"  DASS Depression: M={dass_mean:.2f}\n")

        # Top 3 defining features
        centroid = cluster_profiles.loc[cluster_id]
        top_3 = centroid.abs().sort_values(ascending=False).head(3)
        f.write(f"  Top features:\n")
        for feat, val in top_3.items():
            direction = "high" if centroid[feat] > 0 else "low"
            f.write(f"    - {feat}: {direction} ({centroid[feat]:.2f} SD)\n")

        f.write("\n")

    f.write("STATISTICAL TESTS\n")
    f.write("-" * 80 + "\n")

    if 'gender_male' in demographic_vars:
        f.write(f"Gender × Cluster: χ²={chi2:.3f}, p={p_chi:.4f}")
        if p_chi < 0.05:
            f.write(" → SIGNIFICANT clustering by gender\n")
        else:
            f.write(" → NS\n")

    if 'ucla_total' in feature_df.columns:
        f.write(f"UCLA across clusters: F={f_stat:.3f}, p={p_anova:.4f}")
        if p_anova < 0.05:
            f.write(" → SIGNIFICANT loneliness differences\n")
        else:
            f.write(" → NS\n")

    f.write("\nINTERPRETATION\n")
    f.write("-" * 80 + "\n")

    # Determine dominant pattern
    male_dominant_clusters = [i for i in range(optimal_k)
                             if feature_df[feature_df['cluster']==i]['gender_male'].mean() > 0.7]
    female_dominant_clusters = [i for i in range(optimal_k)
                               if feature_df[feature_df['cluster']==i]['gender_male'].mean() < 0.3]

    if male_dominant_clusters:
        f.write(f"✓ Male-dominant cluster(s): {[cluster_labels_interpretive[i] for i in male_dominant_clusters]}\n")
        f.write("  → Supports gender-specific EF vulnerability patterns\n\n")

    if female_dominant_clusters:
        f.write(f"✓ Female-dominant cluster(s): {[cluster_labels_interpretive[i] for i in female_dominant_clusters]}\n")
        f.write("  → Different vulnerability profile than males\n\n")

    if p_anova < 0.05 if 'ucla_total' in feature_df.columns else False:
        high_ucla_cluster = feature_df.groupby('cluster')['ucla_total'].mean().idxmax()
        f.write(f"✓ Highest UCLA in Cluster {high_ucla_cluster}: {cluster_labels_interpretive[high_ucla_cluster]}\n")
        f.write("  → This EF profile is associated with greater loneliness\n\n")

    f.write("THEORETICAL IMPLICATIONS\n")
    f.write("-" * 80 + "\n")
    f.write("Loneliness does NOT produce a single EF impairment pattern.\n")
    f.write("Instead, vulnerability is HETEROGENEOUS and context-dependent:\n\n")

    if male_dominant_clusters and female_dominant_clusters:
        f.write("  - Males: Show distinct EF profile (likely lapse/variability-based)\n")
        f.write("  - Females: Show alternative profile (possibly hypervigilance/flexibility)\n")
        f.write("  - This aligns with DOUBLE DISSOCIATION from regression analyses\n\n")

    f.write("LIMITATIONS\n")
    f.write("-" * 80 + "\n")
    f.write("- Exploratory: Cluster labels are post-hoc interpretations\n")
    f.write("- Small N limits cluster stability (should replicate in larger sample)\n")
    f.write("- K-means assumes spherical clusters (may miss complex geometries)\n\n")

    f.write("NEXT STEPS\n")
    f.write("-" * 80 + "\n")
    f.write("- Use cluster labels as qualitative support in Discussion\n")
    f.write("- Connect to regression findings (e.g., 'lapse-heavy' = high tau males)\n")
    f.write("- Future: Latent profile analysis (LPA) for probabilistic clustering\n")

print(f"✓ Saved: CLUSTERING_REPORT.txt")

print("\n" + "=" * 80)
print("ANALYSIS COMPLETE")
print("=" * 80)
print(f"\nResults saved to: {OUTPUT_DIR}/")
print("\nKey outputs:")
print("  1. cluster_assignments.csv - Participant cluster membership")
print("  2. cluster_centroids.csv - Mean EF profile per cluster")
print("  3. cluster_demographics.csv - Age/gender/UCLA by cluster")
print("  4. cluster_pca_visualization.png - 2D PCA projection")
print("  5. cluster_profile_heatmap.png - Discriminative features")
print("  6. CLUSTERING_REPORT.txt - Interpretation summary")
print(f"\nKey finding: {optimal_k} distinct EF vulnerability profiles identified")
print("\nInterpretive labels:")
for cluster_id, label in cluster_labels_interpretive.items():
    print(f"  Cluster {cluster_id}: {label}")
