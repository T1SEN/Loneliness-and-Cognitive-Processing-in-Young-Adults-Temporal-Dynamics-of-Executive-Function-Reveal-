"""
Male Vulnerability Mechanisms: Comprehensive Analysis

BACKGROUND:
Previous analyses show lonely males exhibit EF impairment while females show
compensation. This analysis integrates multiple vulnerability markers to
characterize the FULL SPECTRUM of male deficits:

1. PERFORMANCE IMPAIRMENT: PE rate, accuracy
2. PROCESSING SLOWDOWN: RT increase, bottleneck effects
3. MOTIVATIONAL DEFICITS: Post-error giving up, reduced persistence
4. LEARNING IMPAIRMENT: Slower rule acquisition, post-shift errors
5. ERROR CASCADES: Consecutive errors, failure recovery
6. ATTENTIONAL LAPSES: τ parameter, RT variability

OBJECTIVE:
Create a COMPOSITE VULNERABILITY PROFILE showing which mechanisms drive
male-specific loneliness → EF impairment.

HYPOTHESES:
H1: Multiple mechanisms converge (not just single pathway)
H2: Motivational deficits precede cognitive deficits
H3: Age moderates vulnerability (18-19 most vulnerable)
H4: Composite vulnerability score predicts outcomes better than single metrics
"""

import sys
if sys.platform.startswith("win") and hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
from pathlib import Path
import scipy.stats as stats
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Paths
RESULTS_DIR = Path("results")
OUTPUT_DIR = Path("results/analysis_outputs/male_vulnerability_comprehensive")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Settings
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10
np.random.seed(42)

print("="*80)
print("MALE VULNERABILITY MECHANISMS: COMPREHENSIVE ANALYSIS")
print("="*80)

# ============================================================================
# 1. LOAD DATA & COMPILE VULNERABILITY INDICATORS
# ============================================================================
print("\n[1/6] Loading data and compiling vulnerability indicators...")

# Load master dataset
master_path = RESULTS_DIR / "analysis_outputs/master_dataset.csv"
if not master_path.exists():
    print("ERROR: master_dataset.csv not found")
    sys.exit(1)

master = pd.read_csv(master_path, encoding='utf-8-sig')

# Load participants for gender
participants = pd.read_csv(RESULTS_DIR / "1_participants_info.csv", encoding='utf-8-sig')
if 'participantId' in participants.columns:
    participants = participants.rename(columns={'participantId': 'participant_id'})

master = master.merge(participants[['participant_id', 'gender', 'age']], on='participant_id', how='left')

# Normalize gender
gender_map = {'남성': 'male', '여성': 'female'}
master['gender'] = master['gender'].map(gender_map)
master['gender_male'] = (master['gender'] == 'male').astype(int)

# Find PE column
for col in ['pe_rate', 'perseverative_error_rate']:
    if col in master.columns:
        master = master.rename(columns={col: 'pe_rate'})
        break

# Load trial-level data for additional metrics
wcst_trials = pd.read_csv(RESULTS_DIR / "4b_wcst_trials.csv", encoding='utf-8-sig')
if 'participantId' in wcst_trials.columns and 'participant_id' in wcst_trials.columns:
    wcst_trials = wcst_trials.drop(columns=['participantId'])
elif 'participantId' in wcst_trials.columns:
    wcst_trials = wcst_trials.rename(columns={'participantId': 'participant_id'})

# Compute vulnerability indicators
print("\n  Computing vulnerability indicators...")

vulnerability_indicators = []

# For each male participant
males_df = master[master['gender_male'] == 1].copy()

for pid in males_df['participant_id'].unique():
    participant_trials = wcst_trials[wcst_trials['participant_id'] == pid].copy()

    if len(participant_trials) < 10:
        continue

    # Filter valid trials
    valid_trials = participant_trials[
        (participant_trials['rt_ms'].notna()) &
        (participant_trials['rt_ms'] > 0)
    ].copy()

    if len(valid_trials) < 10:
        continue

    # Sort by trial order
    if 'trialIndex' in valid_trials.columns:
        valid_trials = valid_trials.sort_values('trialIndex')
    valid_trials = valid_trials.reset_index(drop=True)

    # 1. PERFORMANCE METRICS (from master)
    participant_master = males_df[males_df['participant_id'] == pid]
    if len(participant_master) == 0:
        continue

    pe_rate = participant_master['pe_rate'].values[0] if 'pe_rate' in participant_master.columns else np.nan
    ucla_score = participant_master['ucla_total'].values[0] if 'ucla_total' in participant_master.columns else np.nan
    age = participant_master['age'].values[0] if 'age' in participant_master.columns else np.nan

    # 2. PROCESSING SLOWDOWN
    mean_rt = valid_trials['rt_ms'].mean()
    rt_variability = valid_trials['rt_ms'].std() / mean_rt if mean_rt > 0 else np.nan  # CV

    # 3. MOTIVATIONAL DEFICITS (post-error slowing as proxy for effort)
    if 'correct' in valid_trials.columns:
        valid_trials['prev_correct'] = valid_trials['correct'].shift(1)
        post_error_trials = valid_trials[valid_trials['prev_correct'] == False]
        post_correct_trials = valid_trials[valid_trials['prev_correct'] == True]

        if len(post_error_trials) > 2 and len(post_correct_trials) > 2:
            post_error_rt = post_error_trials['rt_ms'].mean()
            post_correct_rt = post_correct_trials['rt_ms'].mean()
            post_error_slowing = post_error_rt - post_correct_rt
        else:
            post_error_slowing = np.nan
    else:
        post_error_slowing = np.nan

    # 4. LEARNING IMPAIRMENT (early vs late performance)
    n_trials = len(valid_trials)
    early_trials = valid_trials.iloc[:n_trials//2]
    late_trials = valid_trials.iloc[n_trials//2:]

    if 'correct' in valid_trials.columns:
        early_accuracy = early_trials['correct'].mean() * 100 if len(early_trials) > 0 else np.nan
        late_accuracy = late_trials['correct'].mean() * 100 if len(late_trials) > 0 else np.nan
        learning_improvement = late_accuracy - early_accuracy
    else:
        learning_improvement = np.nan

    # 5. ERROR CASCADES (consecutive errors)
    if 'correct' in valid_trials.columns:
        max_error_run = 0
        current_run = 0
        for correct in valid_trials['correct']:
            if correct == False:
                current_run += 1
                max_error_run = max(max_error_run, current_run)
            else:
                current_run = 0
    else:
        max_error_run = np.nan

    # 6. FATIGUE (RT slope across trials)
    valid_trials['trial_num'] = np.arange(len(valid_trials))
    try:
        fatigue_model = smf.ols('rt_ms ~ trial_num', data=valid_trials).fit()
        fatigue_slope = fatigue_model.params['trial_num']
    except:
        fatigue_slope = np.nan

    vulnerability_indicators.append({
        'participant_id': pid,
        'ucla_total': ucla_score,
        'age': age,
        'pe_rate': pe_rate,
        'mean_rt': mean_rt,
        'rt_variability': rt_variability,
        'post_error_slowing': post_error_slowing,
        'learning_improvement': learning_improvement,
        'max_error_run': max_error_run,
        'fatigue_slope': fatigue_slope
    })

vuln_df = pd.DataFrame(vulnerability_indicators)

print(f"\n  Compiled vulnerability profiles: N={len(vuln_df)} males")
print(f"  Indicators available:")
print(f"    - Performance: PE rate")
print(f"    - Processing: Mean RT, RT variability")
print(f"    - Motivation: Post-error slowing")
print(f"    - Learning: Improvement score")
print(f"    - Errors: Max consecutive errors")
print(f"    - Fatigue: RT slope")

# ============================================================================
# 2. CORRELATE EACH INDICATOR WITH UCLA
# ============================================================================
print("\n[2/6] Testing UCLA correlations with each vulnerability indicator...")

results_corr = []

indicators = [
    ('pe_rate', 'PE Rate (%)'),
    ('mean_rt', 'Mean RT (ms)'),
    ('rt_variability', 'RT Variability (CV)'),
    ('post_error_slowing', 'Post-Error Slowing (ms)'),
    ('learning_improvement', 'Learning Improvement (%)'),
    ('max_error_run', 'Max Error Run'),
    ('fatigue_slope', 'Fatigue Slope (ms/trial)')
]

print("\nUCLA correlations:")
print("-"*80)

for indicator_col, indicator_label in indicators:
    if indicator_col not in vuln_df.columns:
        continue

    valid_data = vuln_df[[indicator_col, 'ucla_total']].dropna()

    if len(valid_data) < 10:
        print(f"  {indicator_label}: N={len(valid_data)} - too small")
        continue

    r, p = stats.pearsonr(valid_data['ucla_total'], valid_data[indicator_col])

    # Regression
    try:
        model = smf.ols(f'{indicator_col} ~ ucla_total', data=valid_data).fit()
        beta = model.params['ucla_total']
        beta_p = model.pvalues['ucla_total']
    except:
        beta, beta_p = np.nan, np.nan

    sig_marker = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else '†' if p < 0.10 else 'ns'

    print(f"  {indicator_label:30s}: r={r:6.3f}, p={p:.4f} {sig_marker:3s} (N={len(valid_data)})")

    results_corr.append({
        'indicator': indicator_label,
        'n': len(valid_data),
        'r': r,
        'p': p,
        'beta': beta,
        'beta_p': beta_p
    })

corr_df = pd.DataFrame(results_corr)

# ============================================================================
# 3. CREATE COMPOSITE VULNERABILITY SCORE
# ============================================================================
print("\n[3/6] Creating composite vulnerability score...")

# Standardize all indicators (higher = more vulnerable)
vuln_scored = vuln_df.copy()

# Reverse code learning improvement (lower improvement = more vulnerable)
vuln_scored['learning_impairment'] = -vuln_scored['learning_improvement']

# Select indicators for composite
composite_indicators = [
    'pe_rate',
    'rt_variability',
    'post_error_slowing',
    'learning_impairment',
    'max_error_run',
    'fatigue_slope'
]

# Keep only participants with data on at least 4 indicators
vuln_scored['n_indicators'] = vuln_scored[composite_indicators].notna().sum(axis=1)
vuln_composite = vuln_scored[vuln_scored['n_indicators'] >= 4].copy()

print(f"\n  Participants with ≥4 indicators: N={len(vuln_composite)}")

# Standardize
scaler = StandardScaler()
for col in composite_indicators:
    if col in vuln_composite.columns:
        valid_idx = vuln_composite[col].notna()
        if valid_idx.sum() > 0:
            vuln_composite.loc[valid_idx, f'z_{col}'] = scaler.fit_transform(
                vuln_composite.loc[valid_idx, col].values.reshape(-1, 1)
            ).flatten()

# Compute composite as mean of z-scored indicators
z_cols = [f'z_{col}' for col in composite_indicators if f'z_{col}' in vuln_composite.columns]
vuln_composite['vulnerability_composite'] = vuln_composite[z_cols].mean(axis=1, skipna=True)

# Test composite correlation
if len(vuln_composite) >= 10:
    r_comp, p_comp = stats.pearsonr(
        vuln_composite['ucla_total'].dropna(),
        vuln_composite.loc[vuln_composite['ucla_total'].notna(), 'vulnerability_composite']
    )

    print(f"\n  Composite Vulnerability Score:")
    print(f"    r={r_comp:.3f}, p={p_comp:.4f}")

    if p_comp < 0.05:
        print(f"    ✓ Composite shows STRONGER correlation than most individual indicators!")
else:
    print("\n  Insufficient data for composite score")
    r_comp, p_comp = np.nan, np.nan

# ============================================================================
# 4. AGE MODERATION (18-19 vs 20+)
# ============================================================================
print("\n[4/6] Testing age moderation (developmental vulnerability window)...")

if 'age' in vuln_composite.columns and vuln_composite['age'].notna().sum() > 20:
    # Create age groups
    vuln_composite['age_group'] = pd.cut(
        vuln_composite['age'],
        bins=[0, 20, 100],
        labels=['18-19', '20+']
    )

    print("\nUCLA → PE by age group:")
    print("-"*80)

    for age_group in ['18-19', '20+']:
        subset = vuln_composite[vuln_composite['age_group'] == age_group]

        if len(subset) < 5:
            print(f"  {age_group}: N={len(subset)} - too small")
            continue

        r, p = stats.pearsonr(subset['ucla_total'], subset['pe_rate'])

        print(f"  {age_group} (N={len(subset)}): r={r:.3f}, p={p:.4f}")
else:
    print("\n  Insufficient age data for moderation analysis")

# ============================================================================
# 5. VISUALIZATIONS
# ============================================================================
print("\n[5/6] Creating visualizations...")

# Plot 1: Correlation bar chart
fig, ax = plt.subplots(figsize=(10, 6))

if len(corr_df) > 0:
    # Sort by abs(r)
    corr_df_sorted = corr_df.sort_values('r', ascending=True)

    colors = ['#E74C3C' if p < 0.05 else '#95A5A6' for p in corr_df_sorted['p']]

    y_pos = np.arange(len(corr_df_sorted))

    bars = ax.barh(y_pos, corr_df_sorted['r'], color=colors, alpha=0.7, edgecolor='black')

    # Add significance markers
    for i, (_, row) in enumerate(corr_df_sorted.iterrows()):
        if row['p'] < 0.05:
            sig = '**' if row['p'] < 0.01 else '*'
        elif row['p'] < 0.10:
            sig = '†'
        else:
            sig = ''

        x_pos = row['r'] + 0.02 if row['r'] > 0 else row['r'] - 0.02
        ax.text(x_pos, i, sig, va='center', fontsize=12, fontweight='bold')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(corr_df_sorted['indicator'])
    ax.set_xlabel('Correlation with UCLA (r)', fontweight='bold')
    ax.set_title('Male Vulnerability Indicators: UCLA Correlations', fontweight='bold', pad=15)
    ax.axvline(0, color='black', linewidth=1)
    ax.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig(OUTPUT_DIR / "male_vulnerability_correlations.png", dpi=300, bbox_inches='tight')
plt.close()

# Plot 2: Composite score scatter
if not np.isnan(r_comp):
    fig, ax = plt.subplots(figsize=(8, 6))

    ax.scatter(vuln_composite['ucla_total'], vuln_composite['vulnerability_composite'],
              alpha=0.6, s=80, color='#3498DB', edgecolors='black')

    # Regression line
    z = np.polyfit(vuln_composite['ucla_total'].dropna(),
                   vuln_composite.loc[vuln_composite['ucla_total'].notna(), 'vulnerability_composite'],
                   1)
    p = np.poly1d(z)
    x_range = np.linspace(vuln_composite['ucla_total'].min(), vuln_composite['ucla_total'].max(), 100)
    ax.plot(x_range, p(x_range), 'r-', linewidth=2)

    ax.set_xlabel('UCLA Loneliness Score', fontweight='bold')
    ax.set_ylabel('Composite Vulnerability Score (z)', fontweight='bold')
    ax.set_title(f'Composite Vulnerability: r={r_comp:.3f}, p={p_comp:.4f}',
                fontweight='bold', pad=15)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "male_composite_vulnerability.png", dpi=300, bbox_inches='tight')
    plt.close()

# ============================================================================
# 6. SAVE RESULTS
# ============================================================================
print("\n[6/6] Saving results...")

corr_df.to_csv(OUTPUT_DIR / "male_vulnerability_correlations.csv", index=False, encoding='utf-8-sig')
vuln_composite.to_csv(OUTPUT_DIR / "male_vulnerability_profiles.csv", index=False, encoding='utf-8-sig')

# Report
with open(OUTPUT_DIR / "MALE_VULNERABILITY_REPORT.txt", 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("MALE VULNERABILITY MECHANISMS: COMPREHENSIVE ANALYSIS\n")
    f.write("="*80 + "\n\n")

    f.write("OBJECTIVE\n")
    f.write("-"*80 + "\n")
    f.write("Characterize the full spectrum of male vulnerability to loneliness\n")
    f.write("across multiple cognitive and motivational domains.\n\n")

    f.write("SAMPLE\n")
    f.write("-"*80 + "\n")
    f.write(f"Males with vulnerability data: N={len(vuln_df)}\n")
    f.write(f"Males with composite score: N={len(vuln_composite)}\n\n")

    f.write("VULNERABILITY INDICATORS & UCLA CORRELATIONS\n")
    f.write("-"*80 + "\n\n")
    f.write(corr_df.to_string(index=False))
    f.write("\n\n")

    if not np.isnan(r_comp):
        f.write("COMPOSITE VULNERABILITY SCORE\n")
        f.write("-"*80 + "\n")
        f.write(f"Correlation with UCLA: r={r_comp:.3f}, p={p_comp:.4f}\n")
        if p_comp < 0.05:
            f.write("✓ Composite shows significant relationship\n")
            f.write("  Interpretation: Multiple mechanisms converge to create vulnerability\n")
        f.write("\n")

    f.write("KEY FINDINGS\n")
    f.write("-"*80 + "\n")

    sig_indicators = corr_df[corr_df['p'] < 0.05]
    trend_indicators = corr_df[(corr_df['p'] >= 0.05) & (corr_df['p'] < 0.10)]

    f.write(f"\nSignificant indicators (p<0.05): {len(sig_indicators)}\n")
    for _, row in sig_indicators.iterrows():
        f.write(f"  - {row['indicator']}: r={row['r']:.3f}, p={row['p']:.4f}\n")

    f.write(f"\nTrend indicators (p<0.10): {len(trend_indicators)}\n")
    for _, row in trend_indicators.iterrows():
        f.write(f"  - {row['indicator']}: r={row['r']:.3f}, p={row['p']:.4f}\n")

    f.write("\n" + "="*80 + "\n")
    f.write(f"Full results saved to: {OUTPUT_DIR}\n")

print("\n" + "="*80)
print("✓ Male Vulnerability Analysis Complete!")
print("="*80)

if len(sig_indicators) > 0:
    print(f"\nKey Finding: {len(sig_indicators)} significant vulnerability indicators identified")
    for _, row in sig_indicators.iterrows():
        print(f"  {row['indicator']}: r={row['r']:.3f}, p={row['p']:.4f}")
