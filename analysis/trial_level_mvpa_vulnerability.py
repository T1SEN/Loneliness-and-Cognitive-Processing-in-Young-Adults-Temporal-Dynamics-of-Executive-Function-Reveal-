"""
Trial-Level MVPA Classifier - Vulnerability Signature Detection

OBJECTIVE:
Build a machine learning classifier to predict "vulnerable moments" at the trial level,
identifying the combination of features that signal impending cognitive failure.

RATIONALE:
- Mean-level analyses may miss momentary vulnerability signatures
- Clinical utility: Predict when intervention is needed (real-time monitoring)
- MVPA reveals multivariate patterns that univariate correlations miss
- SHAP interpretability: Which features drive vulnerability in lonely males?

TARGET VARIABLE (Vulnerable Trial):
Option 1: PE occurrence (WCST)
Option 2: Slow RT (>median + 1SD)
Option 3: Post-error trial (trial after error)

TRIAL-LEVEL FEATURES:
1. Temporal: Trial number, time-on-task, time since last break
2. Performance history: N-back RT, N-back accuracy, running accuracy
3. Variability: Local RT SD (windowed), RT trend (slope)
4. Adaptation: Post-error slowing, post-correct speeding
5. Context: Block number, trial type, recent error count

ANALYSIS PLAN:
1. Extract trial-level features from WCST/Stroop/PRP
2. Define "vulnerable trial" (e.g., PE or slow RT)
3. Train Random Forest classifier (stratified by gender)
4. Compute feature importance + SHAP values
5. Test: Does classifier work better in lonely males?

OUTPUT:
- Classification performance (AUC, accuracy, F1)
- Feature importance rankings
- SHAP summary plots
- Gender differences in predictive features
"""

import sys
if sys.platform.startswith("win") and hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')

import pandas as pd
import numpy as np
from pathlib import Path
import scipy.stats as stats
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Paths
RESULTS_DIR = Path("results")
OUTPUT_DIR = Path("results/analysis_outputs/trial_mvpa")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Settings
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10
np.random.seed(42)

print("="*80)
print("TRIAL-LEVEL MVPA - VULNERABILITY SIGNATURE CLASSIFIER")
print("="*80)

# ============================================================================
# 1. LOAD DATA
# ============================================================================
print("\n[1/5] Loading data...")

# Load WCST trials
wcst_trials = pd.read_csv(RESULTS_DIR / "4b_wcst_trials.csv", encoding='utf-8-sig')

# Normalize participant ID
if 'participantId' in wcst_trials.columns and 'participant_id' not in wcst_trials.columns:
    wcst_trials = wcst_trials.rename(columns={'participantId': 'participant_id'})
elif 'participantId' in wcst_trials.columns and 'participant_id' in wcst_trials.columns:
    wcst_trials = wcst_trials.drop(columns=['participantId'])

# Load master for UCLA and gender
master_path = RESULTS_DIR / "analysis_outputs/master_dataset.csv"
if not master_path.exists():
    print("ERROR: master_dataset.csv not found")
    sys.exit(1)

master = pd.read_csv(master_path, encoding='utf-8-sig')

# Load participants for gender
participants = pd.read_csv(RESULTS_DIR / "1_participants_info.csv", encoding='utf-8-sig')
if 'participantId' in participants.columns:
    participants = participants.rename(columns={'participantId': 'participant_id'})

if 'gender' not in master.columns:
    master = master.merge(participants[['participant_id', 'gender']], on='participant_id', how='left')

# Normalize gender
gender_map = {'남성': 'male', '여성': 'female'}
master['gender'] = master['gender'].map(gender_map)
master['gender_male'] = (master['gender'] == 'male').astype(int)

print(f"  Loaded {len(wcst_trials)} WCST trials")
print(f"  Loaded {len(master)} participants")

# ============================================================================
# 2. EXTRACT TRIAL-LEVEL FEATURES
# ============================================================================
print("\n[2/5] Extracting trial-level features...")

# Parse WCST 'extra' field for isPE
import ast

def parse_wcst_extra(extra_str):
    if not isinstance(extra_str, str):
        return {}
    try:
        return ast.literal_eval(extra_str)
    except (ValueError, SyntaxError):
        return {}

wcst_trials['extra_dict'] = wcst_trials['extra'].apply(parse_wcst_extra)
wcst_trials['isPE'] = wcst_trials['extra_dict'].apply(lambda x: x.get('isPE', False))
wcst_trials['isCorrect'] = wcst_trials['extra_dict'].apply(lambda x: x.get('isCorrect', True))

# Filter valid trials
valid_wcst = wcst_trials[
    (wcst_trials['timeout'] == False) &
    (wcst_trials['rt_ms'] > 0)
].copy()

# Sort by participant and trial order
if 'timestamp' in valid_wcst.columns:
    valid_wcst = valid_wcst.sort_values(['participant_id', 'timestamp'])
elif 'trialIndex' in valid_wcst.columns:
    valid_wcst = valid_wcst.sort_values(['participant_id', 'trialIndex'])

valid_wcst = valid_wcst.reset_index(drop=True)

print(f"  Valid WCST trials: {len(valid_wcst)}")

# Feature engineering
trial_features = []

for pid, group in valid_wcst.groupby('participant_id'):
    if len(group) < 20:
        continue

    # Get participant-level info
    participant_info = master[master['participant_id'] == pid]
    if len(participant_info) == 0:
        continue

    ucla = participant_info['ucla_total'].values[0] if pd.notna(participant_info['ucla_total'].values[0]) else np.nan
    gender_male = participant_info['gender_male'].values[0]

    # Reset group index
    group = group.reset_index(drop=True)
    group['trial_num'] = np.arange(len(group))

    for i, row in group.iterrows():
        # Skip first few trials (need history for features)
        if i < 5:
            continue

        # Feature 1: Trial number (normalized)
        trial_num_norm = i / len(group)

        # Feature 2: Current RT
        current_rt = row['rt_ms']

        # Feature 3: Mean RT last 5 trials
        rt_last_5 = group.loc[max(0, i-5):i-1, 'rt_ms'].mean()

        # Feature 4: RT variability last 5 trials
        rt_sd_last_5 = group.loc[max(0, i-5):i-1, 'rt_ms'].std()

        # Feature 5: Recent accuracy (last 10 trials)
        acc_last_10 = group.loc[max(0, i-10):i-1, 'isCorrect'].mean()

        # Feature 6: Recent PE rate (last 10 trials)
        pe_last_10 = group.loc[max(0, i-10):i-1, 'isPE'].mean()

        # Feature 7: Trial-to-trial RT change
        rt_change = current_rt - group.loc[i-1, 'rt_ms'] if i > 0 else 0

        # Feature 8: Post-error trial (1 if previous trial was error)
        is_post_error = 1 if i > 0 and not group.loc[i-1, 'isCorrect'] else 0

        # Feature 9: Consecutive correct streak
        correct_streak = 0
        for j in range(i-1, -1, -1):
            if group.loc[j, 'isCorrect']:
                correct_streak += 1
            else:
                break

        # Feature 10: UCLA (participant-level)
        ucla_val = ucla

        # Feature 11: Gender
        gender_val = gender_male

        # Target variable: Current trial is PE
        is_vulnerable = row['isPE']

        # Alternative target: Slow RT (>median + 1SD)
        median_rt = group['rt_ms'].median()
        sd_rt = group['rt_ms'].std()
        is_slow_rt = (current_rt > median_rt + sd_rt)

        trial_features.append({
            'participant_id': pid,
            'trial_index': i,
            'trial_num_norm': trial_num_norm,
            'current_rt': current_rt,
            'rt_last_5': rt_last_5,
            'rt_sd_last_5': rt_sd_last_5,
            'acc_last_10': acc_last_10,
            'pe_last_10': pe_last_10,
            'rt_change': rt_change,
            'is_post_error': is_post_error,
            'correct_streak': correct_streak,
            'ucla': ucla_val,
            'gender_male': gender_val,
            'is_PE': is_vulnerable,
            'is_slow_rt': is_slow_rt
        })

trial_df = pd.DataFrame(trial_features)

print(f"  Extracted features for {len(trial_df)} trials")
print(f"  PE trials: {trial_df['is_PE'].sum()} ({trial_df['is_PE'].mean()*100:.1f}%)")
print(f"  Slow RT trials: {trial_df['is_slow_rt'].sum()} ({trial_df['is_slow_rt'].mean()*100:.1f}%)")

# Drop rows with missing values
trial_df = trial_df.dropna()

print(f"  Final trial count after dropping NaNs: {len(trial_df)}")

# ============================================================================
# 3. TRAIN RANDOM FOREST CLASSIFIER
# ============================================================================
print("\n[3/5] Training Random Forest classifiers...")

# Define feature columns
feature_cols = [
    'trial_num_norm', 'current_rt', 'rt_last_5', 'rt_sd_last_5',
    'acc_last_10', 'pe_last_10', 'rt_change', 'is_post_error',
    'correct_streak', 'ucla', 'gender_male'
]

X = trial_df[feature_cols].values
y_pe = trial_df['is_PE'].values

# Check class balance
print(f"\n  Target: PE occurrence")
print(f"    Class 0 (non-PE): {(y_pe == 0).sum()} trials")
print(f"    Class 1 (PE): {(y_pe == 1).sum()} trials")

# Handle class imbalance with class_weight
pe_count = y_pe.sum()
non_pe_count = len(y_pe) - pe_count

if pe_count > 10 and non_pe_count > 10:
    # Train classifier
    clf_pe = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    # Cross-validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(clf_pe, X, y_pe, cv=cv, scoring='roc_auc')

    print(f"\n  PE Classifier Cross-Validation AUC:")
    print(f"    Mean AUC: {cv_scores.mean():.3f} (SD={cv_scores.std():.3f})")

    # Fit on full data for feature importance
    clf_pe.fit(X, y_pe)

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': clf_pe.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\n  Top 5 Most Important Features:")
    for _, row in feature_importance.head(5).iterrows():
        print(f"    {row['feature']}: {row['importance']:.3f}")

    # Save feature importance
    feature_importance.to_csv(OUTPUT_DIR / 'feature_importance_pe.csv', index=False, encoding='utf-8-sig')

else:
    print("\n  Insufficient PE trials for classification")
    feature_importance = None

# ============================================================================
# 4. GENDER-STRATIFIED ANALYSIS
# ============================================================================
print("\n[4/5] Gender-stratified classifier performance...")

# Separate males and females
males_df = trial_df[trial_df['gender_male'] == 1]
females_df = trial_df[trial_df['gender_male'] == 0]

results_by_gender = []

for gender_val, gender_label, subset in [(1, 'Males', males_df), (0, 'Females', females_df)]:
    if len(subset) < 50:
        print(f"\n  {gender_label}: Insufficient trials (N={len(subset)})")
        continue

    X_sub = subset[feature_cols].values
    y_sub = subset['is_PE'].values

    pe_count_sub = y_sub.sum()
    non_pe_count_sub = len(y_sub) - pe_count_sub

    if pe_count_sub < 5 or non_pe_count_sub < 5:
        print(f"\n  {gender_label}: Insufficient PE trials")
        continue

    clf_sub = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

    cv_sub = StratifiedKFold(n_splits=min(5, pe_count_sub), shuffle=True, random_state=42)
    cv_scores_sub = cross_val_score(clf_sub, X_sub, y_sub, cv=cv_sub, scoring='roc_auc')

    print(f"\n  {gender_label} (N={len(subset)} trials, {pe_count_sub} PE):")
    print(f"    Cross-Val AUC: {cv_scores_sub.mean():.3f} (SD={cv_scores_sub.std():.3f})")

    # Fit for feature importance
    clf_sub.fit(X_sub, y_sub)
    feature_imp_sub = pd.DataFrame({
        'feature': feature_cols,
        'importance': clf_sub.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"    Top 3 features:")
    for _, row in feature_imp_sub.head(3).iterrows():
        print(f"      {row['feature']}: {row['importance']:.3f}")

    results_by_gender.append({
        'gender': gender_label,
        'n_trials': len(subset),
        'n_pe': pe_count_sub,
        'auc_mean': cv_scores_sub.mean(),
        'auc_sd': cv_scores_sub.std()
    })

    # Save gender-specific feature importance
    feature_imp_sub.to_csv(OUTPUT_DIR / f'feature_importance_{gender_label.lower()}.csv',
                           index=False, encoding='utf-8-sig')

results_gender_df = pd.DataFrame(results_by_gender)
if len(results_gender_df) > 0:
    results_gender_df.to_csv(OUTPUT_DIR / 'gender_classifier_performance.csv', index=False, encoding='utf-8-sig')

# ============================================================================
# 5. VISUALIZATIONS
# ============================================================================
print("\n[5/5] Creating visualizations...")

if feature_importance is not None:
    # Figure 1: Feature importance
    fig, ax = plt.subplots(figsize=(10, 6))

    sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis', ax=ax)
    ax.set_xlabel('Feature Importance (Random Forest)', fontweight='bold')
    ax.set_ylabel('Feature', fontweight='bold')
    ax.set_title('Feature Importance for Predicting PE (All Participants)', fontweight='bold', pad=15)
    ax.grid(alpha=0.3, axis='x')

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'feature_importance_plot.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("  ✓ Feature importance plot saved")

# Figure 2: Gender comparison
if len(results_by_gender) == 2:
    fig, ax = plt.subplots(figsize=(8, 6))

    x_pos = np.arange(len(results_gender_df))
    ax.bar(x_pos, results_gender_df['auc_mean'], yerr=results_gender_df['auc_sd'],
           color=['#3498DB', '#E74C3C'], edgecolor='black', linewidth=1.5, capsize=5)

    ax.set_xticks(x_pos)
    ax.set_xticklabels(results_gender_df['gender'])
    ax.set_ylabel('Cross-Validation AUC', fontweight='bold')
    ax.set_title('Classifier Performance: Predicting PE by Gender', fontweight='bold', pad=15)
    ax.axhline(0.5, color='gray', linestyle='--', linewidth=2, label='Chance (AUC=0.5)')
    ax.legend(loc='lower right', frameon=True)
    ax.set_ylim([0, 1])
    ax.grid(alpha=0.3, axis='y')

    for i, row in results_gender_df.iterrows():
        ax.text(i, row['auc_mean'] + row['auc_sd'] + 0.02, f"N={row['n_trials']}\n{row['n_pe']} PE",
                ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / 'gender_classifier_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("  ✓ Gender comparison plot saved")

# Generate report
with open(OUTPUT_DIR / "MVPA_VULNERABILITY_REPORT.txt", 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("TRIAL-LEVEL MVPA - VULNERABILITY SIGNATURE CLASSIFIER\n")
    f.write("="*80 + "\n\n")

    f.write("OBJECTIVE\n")
    f.write("-"*80 + "\n")
    f.write("Build a machine learning classifier to predict vulnerable moments at the\n")
    f.write("trial level using multivariate patterns of performance features.\n\n")

    f.write("METHOD\n")
    f.write("-"*80 + "\n")
    f.write("Random Forest classifier with the following features:\n")
    for feat in feature_cols:
        f.write(f"  - {feat}\n")
    f.write("\nTarget: PE occurrence (binary)\n")
    f.write("Cross-validation: 5-fold stratified\n")
    f.write("Class weighting: Balanced (to handle imbalance)\n\n")

    f.write("SAMPLE\n")
    f.write("-"*80 + "\n")
    f.write(f"Total trials analyzed: {len(trial_df)}\n")
    f.write(f"  PE trials: {trial_df['is_PE'].sum()} ({trial_df['is_PE'].mean()*100:.1f}%)\n")
    f.write(f"  Non-PE trials: {(trial_df['is_PE'] == 0).sum()} ({(1 - trial_df['is_PE'].mean())*100:.1f}%)\n\n")

    if feature_importance is not None:
        f.write("OVERALL CLASSIFIER PERFORMANCE\n")
        f.write("-"*80 + "\n")
        f.write(f"Cross-validation AUC: {cv_scores.mean():.3f} (SD={cv_scores.std():.3f})\n\n")

        if cv_scores.mean() > 0.6:
            f.write("✓ Above-chance prediction (AUC > 0.6)\n")
        elif cv_scores.mean() > 0.5:
            f.write("~ Weak prediction (AUC > 0.5 but < 0.6)\n")
        else:
            f.write("✗ At-chance prediction (AUC ≈ 0.5)\n")

        f.write("\nTOP 5 MOST IMPORTANT FEATURES\n")
        f.write("-"*80 + "\n")
        for i, row in feature_importance.head(5).iterrows():
            f.write(f"{i+1}. {row['feature']}: {row['importance']:.3f}\n")
        f.write("\n")

    if len(results_by_gender) > 0:
        f.write("GENDER-SPECIFIC PERFORMANCE\n")
        f.write("-"*80 + "\n")
        for _, row in results_gender_df.iterrows():
            f.write(f"{row['gender']}:\n")
            f.write(f"  Trials: {row['n_trials']}\n")
            f.write(f"  PE trials: {row['n_pe']}\n")
            f.write(f"  AUC: {row['auc_mean']:.3f} (SD={row['auc_sd']:.3f})\n\n")

        if len(results_gender_df) == 2:
            male_auc = results_gender_df[results_gender_df['gender'] == 'Males']['auc_mean'].values[0]
            female_auc = results_gender_df[results_gender_df['gender'] == 'Females']['auc_mean'].values[0]

            if male_auc > female_auc + 0.05:
                f.write("✓ Males show HIGHER predictability (more deterministic vulnerability)\n")
            elif female_auc > male_auc + 0.05:
                f.write("✓ Females show HIGHER predictability\n")
            else:
                f.write("~ Similar predictability across genders\n")

    f.write("\nINTERPRETATION\n")
    f.write("-"*80 + "\n")
    f.write("High AUC = Vulnerability is predictable from recent performance patterns\n")
    f.write("Low AUC = Vulnerability appears random/stochastic\n\n")
    f.write("Clinical utility: Features with high importance can be monitored in real-time\n")
    f.write("to predict when intervention is needed.\n\n")

    f.write("="*80 + "\n")
    f.write(f"Outputs saved to: {OUTPUT_DIR}\n")

print("\n" + "="*80)
print("✓ MVPA VULNERABILITY CLASSIFIER COMPLETE!")
print("="*80)
if feature_importance is not None:
    print(f"\nOverall AUC: {cv_scores.mean():.3f}")
print(f"\nOutputs saved to: {OUTPUT_DIR}")
